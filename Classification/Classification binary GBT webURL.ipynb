{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78fbebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding:utf-8 -*-\n",
    "#-*- coding: cp950 -*-\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "from numpy import concatenate\n",
    "from pandas import concat\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col  \n",
    "import pyspark.sql.types\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from pyspark import since, SparkContext, keyword_only\n",
    "\n",
    "from pyspark.ml.common import _java2py, _py2java\n",
    "from pyspark.ml.wrapper import _jvm\n",
    "from pyspark.ml.util import *\n",
    "from pyspark.ml.wrapper import JavaEstimator, JavaModel\n",
    "from pyspark.ml.param.shared import *\n",
    "from pyspark.ml.common import inherit_doc\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import QuantileDiscretizer, VectorSlicer\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder,VectorAssembler\n",
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "from pyspark.ml.stat import Correlation, ChiSquareTest\n",
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "from pyspark.ml.feature import DCT\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder, Normalizer, StandardScaler\n",
    "from pyspark.ml.feature import MinMaxScaler, MaxAbsScaler, Bucketizer\n",
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.ml.fpm import PrefixSpan\n",
    "\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.clustering import PowerIterationClustering\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import LogisticRegression , OneVsRest\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import FMClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import AFTSurvivalRegression\n",
    "from pyspark.ml.regression import IsotonicRegression\n",
    "from pyspark.ml.regression import FMRegressor\n",
    "\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.classification import LogisticRegressionModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import StreamingLinearRegressionWithSGD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c89c6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/07/26 07:29:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1f4918e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 42522\n",
      "+-------+-----------+--------+-----+------------+-----+------+-----------------+-------------+---------------+-------------------+----------+\n",
      "|Zipcode|ZipCodeType|    City|State|LocationType|  Lat|  Long|         Location|Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|\n",
      "+-------+-----------+--------+-----+------------+-----+------+-----------------+-------------+---------------+-------------------+----------+\n",
      "|  00705|   STANDARD|AIBONITO|   PR|     PRIMARY|18.14|-66.26|NA-US-PR-AIBONITO|        false|           null|               null|      null|\n",
      "|  00610|   STANDARD|  ANASCO|   PR|     PRIMARY|18.28|-67.14|  NA-US-PR-ANASCO|        false|           null|               null|      null|\n",
      "|  00611|     PO BOX| ANGELES|   PR|     PRIMARY|18.28|-66.79| NA-US-PR-ANGELES|        false|           null|               null|      null|\n",
      "|  00612|   STANDARD| ARECIBO|   PR|     PRIMARY|18.45|-66.73| NA-US-PR-ARECIBO|        false|           null|               null|      null|\n",
      "|  00601|   STANDARD|ADJUNTAS|   PR|     PRIMARY|18.16|-66.72|NA-US-PR-ADJUNTAS|        false|           null|               null|      null|\n",
      "+-------+-----------+--------+-----+------------+-----+------+-----------------+-------------+---------------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt= spark.read.format('csv').option(\"header\",'true').load(\"/home/jovyan/work/pySpark/pySpark_james/data/free-zipcode-database-Primary.csv\")\n",
    "\n",
    "print('data:',dt.count()) \n",
    "dt.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "647ad0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: string (nullable = true)\n",
      " |-- Long: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: string (nullable = true)\n",
      " |-- TaxReturnsFiled: string (nullable = true)\n",
      " |-- EstimatedPopulation: string (nullable = true)\n",
      " |-- TotalWages: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ba7771",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`AIBONITO`' given input columns: [City, Decommisioned, EstimatedPopulation, Lat, Location, LocationType, Long, State, TaxReturnsFiled, TotalWages, ZipCodeType, Zipcode];\n'Project ['AIBONITO]\n+- Relation[Zipcode#16,ZipCodeType#17,City#18,State#19,LocationType#20,Lat#21,Long#22,Location#23,Decommisioned#24,TaxReturnsFiled#25,EstimatedPopulation#26,TotalWages#27] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_210/3158061116.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mreplace_question\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AIBONITO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# df=dt.select(['url','urlid','alchemy_category']+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \"\"\"\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`AIBONITO`' given input columns: [City, Decommisioned, EstimatedPopulation, Lat, Location, LocationType, Long, State, TaxReturnsFiled, TotalWages, ZipCodeType, Zipcode];\n'Project ['AIBONITO]\n+- Relation[Zipcode#16,ZipCodeType#17,City#18,State#19,LocationType#20,Lat#21,Long#22,Location#23,Decommisioned#24,TaxReturnsFiled#25,EstimatedPopulation#26,TotalWages#27] csv\n"
     ]
    }
   ],
   "source": [
    "def replace_question(x):\n",
    "    return (\"0\" if x==\"?\" else x)  \n",
    "replace_question= udf(replace_question)\n",
    "\n",
    "dt.select(\"AIBONITO\").show\n",
    "\n",
    "# df=dt.select(['url','urlid','alchemy_category']+\n",
    "#              [replace_question(col(column)).cast(\"double\").alias(column)  \n",
    "#                 for column in dt.columns[3:] ] )\n",
    "\n",
    "# categoryIndexer = StringIndexer(inputCol='alchemy_category', outputCol=\"alchemy_category_Index\")\n",
    "# categoryTransformer=categoryIndexer.fit(df)\n",
    "\n",
    "# df1=categoryTransformer.transform(df)\n",
    "# assemblerInputs =['alchemy_category_Index'] + dt.columns[4:-1]\n",
    "# assembler = VectorAssembler( inputCols=assemblerInputs,outputCol=\"features\")\n",
    "# df3=assembler.transform(df1)\n",
    "\n",
    "# traindf, testdf = df.randomSplit([0.7, 0.3])\n",
    "# traindf.cache()\n",
    "# testdf.cache()\n",
    "\n",
    "# mod=GBTClassifier()\n",
    "# PL= Pipeline(stages=[categoryIndexer,assembler,mod])\n",
    "# PLmod= PL.fit(traindf)\n",
    "# pred=PLmod.transform(testdf)\n",
    "# evaluator = BinaryClassificationEvaluator(\n",
    "#                               rawPredictionCol=\"rawPrediction\",\n",
    "#                               labelCol=\"label\",  \n",
    "#                               metricName=\"areaUnderROC\")\n",
    "\n",
    "# pred = PLmod.transform(testdf)\n",
    "# DescDict = { 0: \" (ephemeral)\",\n",
    "#            1: \" (evergreen)\"}\n",
    "# for data in pred.select('url','prediction').take(5):\n",
    "#     print( \"URL:  \" +str(data[0])+\"\\n\" + \"==>predict:\"+ str(data[1])+ \" note:\"+DescDict[data[1]] +\"\\n\")\n",
    "\n",
    "# auc= evaluator.evaluate(pred)\n",
    "# print('AUC=',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4c380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0ec07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ea074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfb6709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612beb88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
