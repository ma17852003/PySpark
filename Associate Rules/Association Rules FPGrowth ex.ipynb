{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d541214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding:utf-8 -*-\n",
    "#-*- coding: cp950 -*-\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "from numpy import concatenate\n",
    "from pandas import concat\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col  \n",
    "import pyspark.sql.types\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from pyspark import since, SparkContext, keyword_only\n",
    "\n",
    "from pyspark.ml.common import _java2py, _py2java\n",
    "from pyspark.ml.wrapper import _jvm\n",
    "from pyspark.ml.util import *\n",
    "from pyspark.ml.wrapper import JavaEstimator, JavaModel\n",
    "from pyspark.ml.param.shared import *\n",
    "from pyspark.ml.common import inherit_doc\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import QuantileDiscretizer, VectorSlicer\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder,VectorAssembler\n",
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "from pyspark.ml.stat import Correlation, ChiSquareTest\n",
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "from pyspark.ml.feature import DCT\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder, Normalizer, StandardScaler\n",
    "from pyspark.ml.feature import MinMaxScaler, MaxAbsScaler, Bucketizer\n",
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.ml.fpm import PrefixSpan\n",
    "\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.clustering import PowerIterationClustering\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import LogisticRegression , OneVsRest\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import FMClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import AFTSurvivalRegression\n",
    "from pyspark.ml.regression import IsotonicRegression\n",
    "from pyspark.ml.regression import FMRegressor\n",
    "\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.classification import LogisticRegressionModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import StreamingLinearRegressionWithSGD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "110492e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|customer_id|product_id|\n",
      "+-----------+----------+\n",
      "|        370|       154|\n",
      "|        370|        40|\n",
      "|        370|       173|\n",
      "|         41|        55|\n",
      "|         41|       126|\n",
      "|         41|       121|\n",
      "|         41|       321|\n",
      "|        105|        22|\n",
      "|        105|        55|\n",
      "|        105|       133|\n",
      "|        109|        22|\n",
      "|        109|        55|\n",
      "|        109|       133|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "# instantiate Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# make some test data\n",
    "columns = ['customer_id', 'product_id']\n",
    "vals = [\n",
    "     (370, 154),\n",
    "     (370, 40),\n",
    "     (370, 173),\n",
    "     (41, 55),\n",
    "     (41, 126),\n",
    "     (41, 121),\n",
    "     (41, 321),\n",
    "     (105, 22),\n",
    "     (105, 55),\n",
    "     (105, 133),\n",
    "     (109, 22),\n",
    "     (109, 55),\n",
    "     (109, 133)    \n",
    "]\n",
    "\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(vals, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49c4664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------+\n",
      "|customer_id|collect_set(product_id)|\n",
      "+-----------+-----------------------+\n",
      "|        370|         [154, 173, 40]|\n",
      "|         41|    [321, 121, 126, 55]|\n",
      "|        105|          [133, 22, 55]|\n",
      "|        109|          [133, 22, 55]|\n",
      "+-----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe for FPGrowth model input\n",
    "from pyspark.sql.functions import collect_list, col\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql.functions import *\n",
    "transactions = df.groupBy(\"customer_id\")\\\n",
    "      .agg(F.collect_set(\"product_id\"))\n",
    "\n",
    "transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e4670dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5063663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|        items|freq|\n",
      "+-------------+----+\n",
      "|         [55]|   3|\n",
      "|         [22]|   2|\n",
      "|     [22, 55]|   2|\n",
      "|        [133]|   2|\n",
      "|    [133, 22]|   2|\n",
      "|[133, 22, 55]|   2|\n",
      "|    [133, 55]|   2|\n",
      "+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FPGrowth model \n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "mod= FPGrowth(itemsCol=\"collect_set(product_id)\", minSupport=0.5, minConfidence=0.6)\n",
    "model = mod.fit(transactions)\n",
    "\n",
    "# Display frequent itemsets\n",
    "model.freqItemsets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e54732c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------+------------------+-------+\n",
      "|antecedent|consequent|        confidence|              lift|support|\n",
      "+----------+----------+------------------+------------------+-------+\n",
      "| [133, 55]|      [22]|               1.0|               2.0|    0.5|\n",
      "| [133, 22]|      [55]|               1.0|1.3333333333333333|    0.5|\n",
      "|  [22, 55]|     [133]|               1.0|               2.0|    0.5|\n",
      "|      [55]|      [22]|0.6666666666666666|1.3333333333333333|    0.5|\n",
      "|      [55]|     [133]|0.6666666666666666|1.3333333333333333|    0.5|\n",
      "|     [133]|      [22]|               1.0|               2.0|    0.5|\n",
      "|     [133]|      [55]|               1.0|1.3333333333333333|    0.5|\n",
      "|      [22]|      [55]|               1.0|1.3333333333333333|    0.5|\n",
      "|      [22]|     [133]|               1.0|               2.0|    0.5|\n",
      "+----------+----------+------------------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display generated association rules.\n",
    "model.associationRules.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "103d68a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------+----------+\n",
      "|customer_id|collect_set(product_id)|prediction|\n",
      "+-----------+-----------------------+----------+\n",
      "|        370|         [154, 173, 40]|        []|\n",
      "|         41|    [321, 121, 126, 55]| [22, 133]|\n",
      "|        105|          [133, 22, 55]|        []|\n",
      "|        109|          [133, 22, 55]|        []|\n",
      "+-----------+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform examines the input items against all the association rules and summarise the\n",
    "# consequents as prediction\n",
    "model.transform(transactions).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd5b865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8769b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d2f50a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9135b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67388406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3473417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
