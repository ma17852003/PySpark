{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9af483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://notebook.community/donaghhorgan/COMP9033/labs/10%20-%20Association%20rule%20mining\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6650296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/07/23 05:53:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Let's start by importing the packages we'll need. As usual, \n",
    "# we'll import pandas for exploratory analysis, but this week we're also going to use pyspark, \n",
    "# a Python package that wraps Apache Spark and makes its functionality available in Python. \n",
    "# Spark also supports frequent itemset generation using the FPGrowth algorithm, \n",
    "# so we'll import this functionality too.\n",
    "\n",
    "# First, let's initialise a SparkContext object, which will represent our connection to the Spark cluster. \n",
    "# To do this, we must first specify the URL of the master node to connect to. \n",
    "# As we're only running this notebook for demonstration purposes, \n",
    "# we can just run the cluster locally, as follows:\n",
    "\n",
    "sc = pyspark.SparkContext(master='local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8606e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: By specifying master='local[*]', we are instructing Spark to run with as many worker threads \n",
    "#     as there are logical cores available on the host machine. Alternatively, \n",
    "#     we could directly specify the number of threads, e.g. master='local[4]' to run four threads. \n",
    "#     However, we need to make sure to specify at least two threads, so that there is one available for \n",
    "#     resource management and at least one available for data processing.\n",
    "\n",
    "# Spark supports reading from CSV files via its SQLContext object, so let's create this next:\n",
    "\n",
    "sql = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67143fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'groceries.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "682813b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can load the data into a Spark DataFrame (similar to a pandas DataFrame)\n",
    "# using the read.text method of the SQLContext we have created, as follows:\n",
    "    \n",
    "df = sql.read.text(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "186db310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------+\n",
      "|value                                                              |\n",
      "+-------------------------------------------------------------------+\n",
      "|citrus fruit,semi-finished bread,margarine,ready soups             |\n",
      "|tropical fruit,yogurt,coffee                                       |\n",
      "|whole milk                                                         |\n",
      "|pip fruit,yogurt,cream cheese,meat spreads                         |\n",
      "|other vegetables,whole milk,condensed milk,long life bakery product|\n",
      "+-------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Similar to the head method in pandas, we can peek at the first few rows of the data frame via its show method:\n",
    "\n",
    "df.show(5, truncate=False)  # Show the first five rows, and don't truncate the printout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b98df9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='citrus fruit,semi-finished bread,margarine,ready soups')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)  # Take the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2312333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------+\n",
      "|items                                                                                         |\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "|[citrus fruit, semi-finished bread, margarine, ready soups]                                   |\n",
      "|[tropical fruit, yogurt, coffee]                                                              |\n",
      "|[whole milk]                                                                                  |\n",
      "|[pip fruit, yogurt, cream cheese, meat spreads]                                               |\n",
      "|[other vegetables, whole milk, condensed milk, long life bakery product]                      |\n",
      "|[whole milk, butter, yogurt, rice, abrasive cleaner]                                          |\n",
      "|[rolls/buns]                                                                                  |\n",
      "|[other vegetables, UHT-milk, rolls/buns, bottled beer, liquor (appetizer)]                    |\n",
      "|[potted plants]                                                                               |\n",
      "|[whole milk, cereals]                                                                         |\n",
      "|[tropical fruit, other vegetables, white bread, bottled water, chocolate]                     |\n",
      "|[citrus fruit, tropical fruit, whole milk, butter, curd, yogurt, flour, bottled water, dishes]|\n",
      "|[beef]                                                                                        |\n",
      "|[frankfurter, rolls/buns, soda]                                                               |\n",
      "|[chicken, tropical fruit]                                                                     |\n",
      "|[butter, sugar, fruit/vegetable juice, newspapers]                                            |\n",
      "|[fruit/vegetable juice]                                                                       |\n",
      "|[packaged fruit/vegetables]                                                                   |\n",
      "|[chocolate]                                                                                   |\n",
      "|[specialty bar]                                                                               |\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(split('value', ',').alias('items'))  # Split the values column by comma and label the result as 'items'\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43aaffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Association rule mining\n",
    "\n",
    "# Next, let's mine our transaction data to find interesting dependencies between itemsets. While there are a number of approaches available for mining frequently occuring itemsets (e.g. Apriori, Eclat), Spark supports the FPGrowth algorithm directly. To run the algorithm on our set of transactions, we need to specify two parameters:\n",
    "\n",
    "#     minSupport: A minimum support threshold, used to filter out itemsets that don't occur frequently enough.\n",
    "#     minConfidence: A minimum confidence threshold, used to filter out association rules that aren't strong enough.\n",
    "\n",
    "# Let's set the minimum support level at 1% and the minimum confidence level at 10%. We can then train a model using the fit method of the FPGrowth class (in a similar way to using scikit-learn), as follows:\n",
    "\n",
    "algorithm = FPGrowth(minSupport=0.01, minConfidence=0.1)\n",
    "\n",
    "model = algorithm.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad16e412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+----+\n",
      "|items                               |freq|\n",
      "+------------------------------------+----+\n",
      "|[canned vegetables]                 |106 |\n",
      "|[pork]                              |567 |\n",
      "|[pork, rolls/buns]                  |111 |\n",
      "|[pork, other vegetables]            |213 |\n",
      "|[pork, other vegetables, whole milk]|100 |\n",
      "|[pork, soda]                        |117 |\n",
      "|[pork, root vegetables]             |134 |\n",
      "|[pork, whole milk]                  |218 |\n",
      "|[frozen fish]                       |115 |\n",
      "|[roll products]                     |101 |\n",
      "+------------------------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# We can extract the most frequent itemsets from the model using its freqItemsets attribute, \n",
    "# which is just another data frame object that we can call show on:\n",
    "\n",
    "model.freqItemsets.show(10, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "474172f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+\n",
      "|items             |freq|\n",
      "+------------------+----+\n",
      "|[whole milk]      |2513|\n",
      "|[other vegetables]|1903|\n",
      "|[rolls/buns]      |1809|\n",
      "|[soda]            |1715|\n",
      "|[yogurt]          |1372|\n",
      "|[bottled water]   |1087|\n",
      "|[root vegetables] |1072|\n",
      "|[tropical fruit]  |1032|\n",
      "|[shopping bags]   |969 |\n",
      "|[sausage]         |924 |\n",
      "+------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can print the top ten most frequent itemsets by sorting the data frame before calling show, as follows:\n",
    "\n",
    "model.freqItemsets.sort('freq', ascending=False).show(10, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa7065a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can determing the total number of frequent itemsets found by counting the rows in the data \n",
    "# frame via its count method:\n",
    "\n",
    "model.freqItemsets.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6704bcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+------------------+------------------+------------------+--------------------+\n",
      "|antecedent                       |consequent        |confidence        |lift              |support             |\n",
      "+---------------------------------+------------------+------------------+------------------+--------------------+\n",
      "|[citrus fruit, root vegetables]  |[other vegetables]|0.5862068965517241|3.0296084222733612|0.010371123538383325|\n",
      "|[tropical fruit, root vegetables]|[other vegetables]|0.5845410628019324|3.020999134344196 |0.012302999491611592|\n",
      "|[curd, yogurt]                   |[whole milk]      |0.5823529411764706|2.27912502048173  |0.010066090493136757|\n",
      "|[butter, other vegetables]       |[whole milk]      |0.5736040609137056|2.244884973770909 |0.011489578037620742|\n",
      "|[tropical fruit, root vegetables]|[whole milk]      |0.5700483091787439|2.2309690094599866|0.011997966446365024|\n",
      "|[root vegetables, yogurt]        |[whole milk]      |0.562992125984252 |2.2033535849801504|0.014539908490086425|\n",
      "|[domestic eggs, other vegetables]|[whole milk]      |0.5525114155251142|2.1623357627097084|0.012302999491611592|\n",
      "|[whipped/sour cream, yogurt]     |[whole milk]      |0.5245098039215687|2.052747282757114 |0.010879511947127605|\n",
      "|[root vegetables, rolls/buns]    |[whole milk]      |0.5230125523012552|2.04688756541299  |0.012709710218607015|\n",
      "|[pip fruit, other vegetables]    |[whole milk]      |0.5175097276264592|2.0253514409893456|0.013523131672597865|\n",
      "+---------------------------------+------------------+------------------+------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As can be seen, the FPGrowth algorithm has identified 332 frequent itemsets in the transaction history.\n",
    "# We can extract association rules from the model using its associationRules attribute, which is a further \n",
    "# data frame object that we can call show on. As above, we can sort the data frame according to the \n",
    "# computed confidence level to show the most significant rules first.\n",
    "\n",
    "model.associationRules.sort('confidence', ascending=False).show(10, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02b9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
