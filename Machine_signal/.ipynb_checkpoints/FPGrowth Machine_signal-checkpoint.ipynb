{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d541214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding:utf-8 -*-\n",
    "#-*- coding: cp950 -*-\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "from numpy import concatenate\n",
    "from pandas import concat\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col  \n",
    "import pyspark.sql.types\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from pyspark import since, SparkContext, keyword_only\n",
    "\n",
    "from pyspark.ml.common import _java2py, _py2java\n",
    "from pyspark.ml.wrapper import _jvm\n",
    "from pyspark.ml.util import *\n",
    "from pyspark.ml.wrapper import JavaEstimator, JavaModel\n",
    "from pyspark.ml.param.shared import *\n",
    "from pyspark.ml.common import inherit_doc\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import QuantileDiscretizer, VectorSlicer\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder,VectorAssembler\n",
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "from pyspark.ml.stat import Correlation, ChiSquareTest\n",
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "from pyspark.ml.feature import DCT\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder, Normalizer, StandardScaler\n",
    "from pyspark.ml.feature import MinMaxScaler, MaxAbsScaler, Bucketizer\n",
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.ml.fpm import PrefixSpan\n",
    "\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml.clustering import PowerIterationClustering\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import LogisticRegression , OneVsRest\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import FMClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.regression import AFTSurvivalRegression\n",
    "from pyspark.ml.regression import IsotonicRegression\n",
    "from pyspark.ml.regression import FMRegressor\n",
    "\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.classification import LogisticRegressionModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import StreamingLinearRegressionWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71bb54dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/07/26 07:14:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "366b0652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|   machine|signal|\n",
      "+----------+------+\n",
      "|machine370|  A154|\n",
      "|machine370|  B040|\n",
      "+----------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "df= spark.read.format('csv').option(\"header\",'true').load(\"Machine_signal.csv\")\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08d99a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(machine='machine370', signal='A154')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)  # Take the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c877cf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- machine: string (nullable = true)\n",
      " |-- signal: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7939eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|   machine| collect_set(signal)|\n",
      "+----------+--------------------+\n",
      "|machine370|  [A154, B040, A173]|\n",
      "|machine109|  [A133, B022, B055]|\n",
      "|machine105|  [A133, B022, B055]|\n",
      "| machine41|[A126, B055, C321...|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe for FPGrowth model input\n",
    "from pyspark.sql.functions import collect_list, col\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql.functions import *\n",
    "transactions = df.groupBy(\"machine\")\\\n",
    "      .agg(F.collect_set(\"signal\"))\n",
    "\n",
    "transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "663152f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+\n",
      "|items             |freq|\n",
      "+------------------+----+\n",
      "|[B055]            |3   |\n",
      "|[B022]            |2   |\n",
      "|[B022, B055]      |2   |\n",
      "|[A133]            |2   |\n",
      "|[A133, B022]      |2   |\n",
      "|[A133, B022, B055]|2   |\n",
      "|[A133, B055]      |2   |\n",
      "|[C321]            |1   |\n",
      "|[C321, B055]      |1   |\n",
      "|[A173]            |1   |\n",
      "+------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "mod= FPGrowth(itemsCol=\"collect_set(signal)\",minSupport=0.01, minConfidence=0.1)\n",
    "#mod= FPGrowth(minSupport=0.01, minConfidence=0.1)\n",
    "model = mod.fit(transactions)\n",
    "\n",
    "model.freqItemsets.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e94ae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+\n",
      "|items             |freq|\n",
      "+------------------+----+\n",
      "|[B055]            |3   |\n",
      "|[B022, B055]      |2   |\n",
      "|[B022]            |2   |\n",
      "|[A133, B022, B055]|2   |\n",
      "|[A133, B055]      |2   |\n",
      "|[A133]            |2   |\n",
      "|[A133, B022]      |2   |\n",
      "|[C321]            |1   |\n",
      "|[C321, B055]      |1   |\n",
      "|[A173]            |1   |\n",
      "+------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 72:=====================================================>(198 + 2) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.freqItemsets.sort('freq', ascending=False).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56c4bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.freqItemsets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5831927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:==========================================>           (156 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+------------------+-------+\n",
      "|antecedent  |consequent|confidence|lift              |support|\n",
      "+------------+----------+----------+------------------+-------+\n",
      "|[A133]      |[B022]    |1.0       |2.0               |0.5    |\n",
      "|[A133]      |[B055]    |1.0       |1.3333333333333333|0.5    |\n",
      "|[A126, C321]|[A121]    |1.0       |4.0               |0.25   |\n",
      "|[A126, C321]|[B055]    |1.0       |1.3333333333333333|0.25   |\n",
      "|[A154, A173]|[B040]    |1.0       |4.0               |0.25   |\n",
      "|[A126, A121]|[C321]    |1.0       |4.0               |0.25   |\n",
      "|[A126, A121]|[B055]    |1.0       |1.3333333333333333|0.25   |\n",
      "|[A154, B040]|[A173]    |1.0       |4.0               |0.25   |\n",
      "|[A121, C321]|[B055]    |1.0       |1.3333333333333333|0.25   |\n",
      "|[A121, C321]|[A126]    |1.0       |4.0               |0.25   |\n",
      "+------------+----------+----------+------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.associationRules.sort('confidence', ascending=False).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da18f951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|   machine| collect_set(signal)|        prediction|\n",
      "+----------+--------------------+------------------+\n",
      "|machine370|  [A154, B040, A173]|                []|\n",
      "|machine109|  [A133, B022, B055]|[C321, A121, A126]|\n",
      "|machine105|  [A133, B022, B055]|[C321, A121, A126]|\n",
      "| machine41|[A126, B055, C321...|      [B022, A133]|\n",
      "+----------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform examines the input items against all the association rules and summarise the\n",
    "# consequents as prediction\n",
    "model.transform(transactions).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a1015e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906709d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19949a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110492e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c4664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5063663c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d68a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd5b865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8769b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d2f50a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9135b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67388406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3473417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
